
----------------------------
Python performances Notes
----------------------------

1. Measuring performance
    Various Approch to understand performances 
        1. time() function
        2. timeit module 
        3. pytest-benchmark plugin for pytest  

        mprofile and cprofile can be used
        mprofile adds significant overhead and easy to extend whereas cProfile adds less overhead and general purpose 

    Usage of time module.
        mpatel@blr-mpd67 demos % cat empty_loop.py 
        import time


        def heavy_work():
            for _ in range(100_000_000):
                pass

        start_time = time.time()
        heavy_work()
        end_time = time.time()
        print(f'Duration: {end_time - start_time}')
        mpatel@blr-mpd67 demos % python3 empty_loop.py 
        Duration: 2.1028521060943604

    Usage of timeit module.
        % python3 -m timeit -c "for _ in range(100_000_000): pass"
        1 loop, best of 5: 2.02 sec per loop

    Usage of pytest 
        % pytest test_empty_loop.py
        =================================================================================================================== test session starts ====================================================================================================================
        platform darwin -- Python 3.11.3, pytest-7.4.3, pluggy-1.3.0
        benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
        rootdir: /Users/mpatel/git/PycharmProjects/pluralSightDoc/python-3-performance/02/demos
        plugins: benchmark-4.0.0
        collected 1 item                                                                                                                                                                                                                                           

        test_empty_loop.py .                                                                                                                                                                                                                                 [100%]


        ----------------------------------------------- benchmark: 1 tests -----------------------------------------------
        Name (time in s)                 Min     Max    Mean  StdDev  Median     IQR  Outliers     OPS  Rounds  Iterations
        ------------------------------------------------------------------------------------------------------------------
        test_benchmark_heavy_work     1.9996  2.0830  2.0280  0.0365  2.0062  0.0548       1;0  0.4931       5           1
        ------------------------------------------------------------------------------------------------------------------

        Legend:
        Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
        OPS: Operations Per Second, computed as 1 / Mean
        ==================================================================================================================== 1 passed in 17.23s ====================================================================================================================

        % cat test_empty_loop.py
        from empty_loop import heavy_work

        def test_benchmark_heavy_work(benchmark):
            benchmark(heavy_work)

    Profiler out puts and overall cProfile is better choice as its faster. 
        % python3 -m cProfile sum_loop.py 
        Duration:  34.82 seconds
                100000007 function calls in 34.819 seconds

        Ordered by: cumulative time

        ncalls  tottime  percall  cumtime  percall filename:lineno(function)
                1    0.000    0.000   34.819   34.819 {built-in method builtins.exec}
                1    0.000    0.000   34.819   34.819 sum_loop.py:1(<module>)
                1   24.542   24.542   34.819   34.819 sum_loop.py:4(heavy_work)
        100000000   10.276    0.000   10.276    0.000 sum_loop.py:9(do_stuff)
                1    0.000    0.000    0.000    0.000 {built-in method builtins.print}
                2    0.000    0.000    0.000    0.000 {built-in method time.time}
                1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}

        % python3 -m profile sum_loop.py 
        Duration:  545.39 seconds
                100000008 function calls in 230.181 seconds

        Ordered by: standard name

        ncalls  tottime  percall  cumtime  percall filename:lineno(function)
                1    0.000    0.000  230.168  230.168 :0(exec)
                1    0.000    0.000    0.000    0.000 :0(print)
                1    0.013    0.013    0.013    0.013 :0(setprofile)
                2    0.000    0.000    0.000    0.000 :0(time)
                1    0.000    0.000  230.181  230.181 profile:0(<code object <module> at 0x1032ab210, file "sum_loop.py", line 1>)
                0    0.000             0.000          profile:0(profiler)
                1    0.000    0.000  230.168  230.168 sum_loop.py:1(<module>)
                1  122.164  122.164  230.168  230.168 sum_loop.py:4(heavy_work)
        100000000  108.004    0.000  108.004    0.000 sum_loop.py:9(do_stuff)

    Third party profilers --> Line_profiler, Py-spy, Scalene, Yappi, Memory_profiler

        Line_profiler:
            % pip3 install line_profiler
            % kernprof -lv large_function.py 
            Do something
            Do something
            Do something
            Do something
            Do something
            Duration: 107.20122194290161
            Wrote profile results to large_function.py.lprof
            Timer unit: 1e-06 s

            Total time: 46.9882 s
            File: large_function.py
            Function: heavy_work at line 4

            Line #      Hits         Time  Per Hit   % Time  Line Contents
            ==============================================================
                4                                           @profile
                5                                           def heavy_work():
                6         1         33.0     33.0      0.0      print('Do something')
                7         1          5.0      5.0      0.0      print('Do something')
                8         1          3.0      3.0      0.0      print('Do something')
                9                                           
                10 100000001   25940663.0      0.3     55.2      for _ in range(100_000_000):
                11 100000000   21047451.0      0.2     44.8          pass
                12                                           
                13         1         26.0     26.0      0.0      print('Do something')
                14         1         23.0     23.0      0.0      print('Do something')

            % cat large_function.py
            import time


            @profile
            def heavy_work():
                print('Do something')
                print('Do something')
                print('Do something')

                for _ in range(100_000_000):
                    pass

                print('Do something')
                print('Do something')

            start_time = time.time()
            heavy_work()
            end_time = time.time()
            print(f'Duration: {end_time - start_time}')

        Memory_profiler
            % pip3 install memory_profiler

            % cat memory_intensive.py 
            @profile
            def create_big_list():
                return 10_000_000 * [0]

            @profile
            def create_huge_list():
                return 30_000_000 * [0]

            create_big_list()
            create_huge_list()

            % python3 -m memory_profiler memory_intensive.py 
            Filename: memory_intensive.py

            Line #    Mem usage    Increment  Occurrences   Line Contents
            =============================================================
                1   43.707 MiB   43.707 MiB           1   @profile
                2                                         def create_big_list():
                3  120.004 MiB   76.297 MiB           1       return 10_000_000 * [0]


            Filename: memory_intensive.py

            Line #    Mem usage    Increment  Occurrences   Line Contents
            =============================================================
                5  120.008 MiB  120.008 MiB           1   @profile
                6                                         def create_huge_list():
                7  348.891 MiB  228.883 MiB           1       return 30_000_000 * [0]

    Snakeviz - graphical profiler 
        % pip3 install matplotlib

        demos %  mprof run memory_intensive.py 
        mprof: Sampling memory every 0.1s
        running new process
        running as a Python program...
        demos % mprof plot --output memory_intensive.jpg
        Using last profile data.
        demos % open memory_intensive.jpg 

        mpatel@blr-mpd67 demos % pip3 install snakeviz
        mpatel@blr-mpd67 demos % python3 -m cProfile -o sum_loop.prof sum_loop.py 
        Duration:  33.61 seconds
        mpatel@blr-mpd67 demos % snakeviz sum_loop.prof 
        snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit
        http://127.0.0.1:8080/snakeviz/%2FUsers%2Fmpatel%2Fgit%2FPycharmProjects%2FpluralSightDoc%2Fpython-3-performance%2F02%2Fdemos%2Fsum_loop.prof



2. Using right Data structures
    Big O Notation
        #Constant O(1)
            amounts = [3, 5, 7]
            double = amount[0] * 2
        #Linear O(n)
            sum = 0 
            for i in amounts:
                sum += i
        
        O(n^2)      O(n log(n))     O(n)    O(log n)    O(1)
        -------------------------------------------------------->
        Very slow                                       Very Fast

        mpatel@blr-mpd67 demos % python3 big_o.py 
        Duration double: 9.5367431640625e-07
        Duration sum: 6.794929504394531e-05
        Ratio of durations: 71.25
        mpatel@blr-mpd67 demos % cat big_o.py 
        import random
        import time


        def double_first_amount(amounts):
            return amounts[0] * 2

        def sum_odd_amounts(amounts):
            sum = 0
            for a in amounts:
                if a % 2:
                    sum += a
            return sum

        randomAmounts = [random.randint(1, 100) for _ in range(1000)]

        start_time = time.time()
        double_first_amount(randomAmounts)
        double_duration = time.time() - start_time

        start_time = time.time()
        sum_odd_amounts(randomAmounts)
        sum_duration = time.time() - start_time

        print(f'Duration double: {double_duration}')
        print(f'Duration sum: {sum_duration}')
        print(f'Ratio of durations: {sum_duration/double_duration:.2f}')
    
    List and Arrays are for holding more values.
        List
            Ordered Collections
            Allow Mixed types but always save same types so it allows python to do Optimization
            Implemented as resizable Arrays
            Very Fast -- O(1)
                Getting values
                Setting values
                Appending values
            Slow -- O(n)
                Finding a item
                removing a item
            Memory allocation
                Extra room for future appends
                Old list is copied to new list for larger values.
                keep an eye on multiple appends
        Arrays
            built-in Arrays
                Compact data storage, only for certain types and less popular
            NumPy Arrays
                Numeric computations items of different types and very popular
            
            #Demo to show numpy array are much faster then default list.
            mpatel@blr-mpd67 demos % cat list_array.py
            import numpy

            def double_list(size):
                initial_list = list(range(size))
                return [2 * i for i in initial_list]

            def double_array(size):
                initial_array = numpy.arange(size)
                return 2 * initial_array

            double_list(1_000_000)
            double_array(1_000_000) 
            mpatel@blr-mpd67 demos % python3 -m cProfile -o list_array.prof list_array.py 
            mpatel@blr-mpd67 demos % snakeviz list_array.prof 
            snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit
            http://127.0.0.1:8080/snakeviz/%2FUsers%2Fmpatel%2Fgit%2FPycharmProjects%2FpluralSightDoc%2Fpython-3-performance%2F03%2Fdemos%2Flist_array.prof
            ^C
            Bye!
            mpatel@blr-mpd67 demos %
    
    Sets and tuples
        Sets
            unordered collections of unique immutable items however sets are mutable like adding new items to set.
            Very Fast O(1):
                Adding
                Deleting
                Membership checking
            Slow - O(n):
                removing duplicate.
        Tuples
            immutable, Ordered collection, Fixed content so they are memory efficient 
        
        # Demo 
        # Set needs less memory then List but tuple is so much efficient
        # search inside set is very efficient compared to tuple.
        mpatel@blr-mpd67 demos % python3 -m memory_profiler set_tuple.py
        Filename: set_tuple.py

        Line #    Mem usage    Increment  Occurrences   Line Contents
        =============================================================
            11   42.738 MiB   42.738 MiB           1   @profile
            12                                         def main():
            13   42.738 MiB    0.000 MiB           1       SIZE = 1_000_000
            14                                         
            15   80.992 MiB   38.254 MiB           1       big_list = list(range(SIZE))
            16   88.625 MiB    7.633 MiB           1       big_tuple = tuple(big_list)
            17  151.156 MiB   62.531 MiB           1       big_set = set(big_list)
            18                                         
            19  151.188 MiB    0.031 MiB        1003       items_to_find = [random.randint(0, SIZE) for _ in range(1000)]
            20                                         
            21  151.188 MiB    0.000 MiB           1       search(items_to_find, big_tuple)
            22  151.188 MiB    0.000 MiB           1       search(items_to_find, big_set)
            23  151.188 MiB    0.000 MiB           1       search(items_to_find, big_list)


        mpatel@blr-mpd67 demos % kernprof -lv set_tuple.py
        Wrote profile results to set_tuple.py.lprof
        Timer unit: 1e-06 s

        Total time: 10.4118 s
        File: set_tuple.py
        Function: main at line 11

        Line #      Hits         Time  Per Hit   % Time  Line Contents
        ==============================================================
            11                                           @profile
            12                                           def main():
            13         1          0.0      0.0      0.0      SIZE = 1_000_000
            14                                           
            15         1      27904.0  27904.0      0.3      big_list = list(range(SIZE))
            16         1       6032.0   6032.0      0.1      big_tuple = tuple(big_list)
            17         1      46716.0  46716.0      0.4      big_set = set(big_list)
            18                                           
            19         1       3480.0   3480.0      0.0      items_to_find = [random.randint(0, SIZE) for _ in range(1000)]
            20                                           
            21         1    5145493.0    5e+06     49.4      search(items_to_find, big_tuple)
            22         1        442.0    442.0      0.0      search(items_to_find, big_set)
            23         1    5181765.0    5e+06     49.8      search(items_to_find, big_list)

        mpatel@blr-mpd67 demos % cat set_tuple.py
        import random


        def search(items, collection):
            count = 0
            for i in items:
                if i in collection:
                    count += 1
            return count

        @profile
        def main():
            SIZE = 1_000_000

            big_list = list(range(SIZE))
            big_tuple = tuple(big_list)
            big_set = set(big_list)

            items_to_find = [random.randint(0, SIZE) for _ in range(1000)]

            search(items_to_find, big_tuple)
            search(items_to_find, big_set)
            search(items_to_find, big_list)

        main()
        mpatel@blr-mpd67 demos %
    
    Queues and Deques
        Queue
            From queue module, simple queue data structure - FIFO, Specialized for multithreading and few operations
        Deque
            From collections module, Double eneded queue, FIFO as well LIFO (stack), multithreading support and more operations
            Deque                       | List
            Slow access by index - O(n) | Fast access by index - O(1)
            Fast append and pop at end  | Fast append and pop at end
            Fast append and pop at start| slow append and pop at start

        # Demo
        # avoid append at start of List as it takes lot of time 
        # deque are best option for append at start. 
            mpatel@blr-mpd67 demos % cat deque.py 
            from collections import deque


            @profile
            def main():
                SIZE = 100_000

                big_list = list(range(SIZE))
                big_queue = deque(big_list)

                while big_list:
                    big_list.pop()
                while big_queue:
                    big_queue.pop()
                
                big_list = list(range(SIZE))
                big_queue = deque(big_list)

                while big_list:
                    big_list.pop(0)
                while big_queue:
                    big_queue.popleft()


            main()
            mpatel@blr-mpd67 demos % kernprof -lv deque.py 
            Wrote profile results to deque.py.lprof
            Timer unit: 1e-06 s

            Total time: 1.11005 s
            File: deque.py
            Function: main at line 4

            Line #      Hits         Time  Per Hit   % Time  Line Contents
            ==============================================================
                4                                           @profile
                5                                           def main():
                6         1          0.0      0.0      0.0      SIZE = 100_000
                7                                           
                8         1       3062.0   3062.0      0.3      big_list = list(range(SIZE))
                9         1        977.0    977.0      0.1      big_queue = deque(big_list)
                10                                           
                11    100001      26349.0      0.3      2.4      while big_list:
                12    100000      33174.0      0.3      3.0          big_list.pop()
                13    100001      24634.0      0.2      2.2      while big_queue:
                14    100000      32068.0      0.3      2.9          big_queue.pop()
                15                                               
                16         1       2024.0   2024.0      0.2      big_list = list(range(SIZE))
                17         1        762.0    762.0      0.1      big_queue = deque(big_list)
                18                                           
                19    100001      27748.0      0.3      2.5      while big_list:
                20    100000     903668.0      9.0     81.4          big_list.pop(0)
                21    100001      24354.0      0.2      2.2      while big_queue:
                22    100000      31233.0      0.3      2.8          big_queue.popleft()

            mpatel@blr-mpd67 demos % 
    
    Dictionaries
        collections of key-value pair
        mutable (add, remove or update possible)
        keys are unique 
        need keys to access value
        Keys must be hashable.
        Very popular 
        Very fast - O(1) - Getting, Setting or Deleting
        Slow - O(n) - Worst case scenarios

        # Demo to see benefit compared to dict. 99.2% time spent for search in List.
            mpatel@blr-mpd67 demos % cat dictionary.py 
            import random

            def search_list(big_list, items):
                count = 0
                for item in items:
                    for order in big_list:
                        if item == order[0]:
                            count += 1
                return count

            def search_dictionary(some_dictionary, items):
                count = 0
                for item in items:
                    if item in some_dictionary:
                        count += 1
                return count

            @profile
            def main():
                SIZE = 100_000

                big_list = []
                big_dictionary = {}

                for i in range(SIZE):
                    big_list.append([i, 2 * i, i * i])
                    big_dictionary[i] = [2 * i, i * i]

                orders_to_search = [random.randint(0, SIZE) for _ in range(1000)]
                search_list(big_list, orders_to_search)
                search_dictionary(big_dictionary, orders_to_search)

            main()
            mpatel@blr-mpd67 demos % kernprof -lv dictionary.py 
            Wrote profile results to dictionary.py.lprof
            Timer unit: 1e-06 s

            Total time: 22.4539 s
            File: dictionary.py
            Function: main at line 18

            Line #      Hits         Time  Per Hit   % Time  Line Contents
            ==============================================================
                18                                           @profile
                19                                           def main():
                20         1          1.0      1.0      0.0      SIZE = 100_000
                21                                           
                22         1          0.0      0.0      0.0      big_list = []
                23         1          0.0      0.0      0.0      big_dictionary = {}
                24                                           
                25    100001      30192.0      0.3      0.1      for i in range(SIZE):
                26    100000      89238.0      0.9      0.4          big_list.append([i, 2 * i, i * i])
                27    100000      54191.0      0.5      0.2          big_dictionary[i] = [2 * i, i * i]
                28                                           
                29         1       2778.0   2778.0      0.0      orders_to_search = [random.randint(0, SIZE) for _ in range(1000)]
                30         1   22276672.0    2e+07     99.2      search_list(big_list, orders_to_search)
                31         1        779.0    779.0      0.0      search_dictionary(big_dictionary, orders_to_search)

            mpatel@blr-mpd67 demos % 

    Named Tuples, Data classe
        Named Tuples
            Tuples with named field, better readability, immutable, great at storing read only data like database reads, and functional agrument and memory efficient
        Data class 
            stores data with class without lots of boilerplate code
            Decorate class with @dataclass
            Optional immutability by default mutable
        
        # Demo : Dictionaries are fastest to created and named tuple take max time. (use -s to Measuring required part only ) where as accessing dataclass are fastest
            mpatel@blr-mpd67 demos % python3 -m timeit '{"order_id":1}'
            5000000 loops, best of 5: 46.7 nsec per loop
            mpatel@blr-mpd67 demos % python3 -m timeit -s 'from collections import namedtuple; Order=namedtuple("Order", "order_id")' 'Order(1)'
            1000000 loops, best of 5: 228 nsec per loop
            mpatel@blr-mpd67 demos % python3 -m timeit -s """                                                                                   
            dquote> from dataclasses import dataclass
            dquote> @dataclass
            dquote> class order:
            dquote>   order_id: int
            dquote> """ 'order(1)'
            2000000 loops, best of 5: 156 nsec per loop
            mpatel@blr-mpd67 demos %

            #accessing objects 
            mpatel@blr-mpd67 demos % python3 -m timeit -s 'order={"order_id":1}' 'order["order_id"]'
            10000000 loops, best of 5: 24.6 nsec per loop
            mpatel@blr-mpd67 demos % python3 -m timeit -s 'from collections import namedtuple; Order=namedtuple("Order", "order_id"); Order(1)' 'Order.order_id'
            10000000 loops, best of 5: 25.4 nsec per loop            
            mpatel@blr-mpd67 demos % python3 -m timeit -s """
            from dataclasses import dataclass
            @dataclass
            class order:
            order_id: int
            order=order(1)""" 'order.order_id'
            20000000 loops, best of 5: 14 nsec per loop
            mpatel@blr-mpd67 demos %

3. Optimizing python code


4. Using more threads


5. Using Asynchronous code


6. Using more processes 